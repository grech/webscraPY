# Web Scraper con Clean Architecture en Python

Este proyecto es un **Web Scraper** desarrollado en Python siguiendo las mejores prÃ¡cticas de **Clean Code** y **Clean Architecture**. Permite extraer el tÃ­tulo de pÃ¡ginas web y almacenar los resultados en un archivo JSON.

## ðŸ“‹ Tabla de Contenidos

-   [CaracterÃ­sticas](#-caracterÃ­sticas)
-   [TecnologÃ­as](#-tecnologÃ­as)
-   [InstalaciÃ³n](#-instalaciÃ³n)
-   [Uso](#-uso)
-   [Estructura del Proyecto](#-estructura-del-proyecto)
-   [Pruebas Unitarias](#-pruebas-unitarias)
-   [Almacenamiento de Datos](#-almacenamiento-de-datos)
-   [PrÃ³ximos Pasos](#-prÃ³ximos-pasos)
-   [Contribuir](#-contribuir)
-   [Licencia](#-licencia)

## ðŸ”¹ CaracterÃ­sticas

-   Arquitectura desacoplada (Clean Architecture)
-   Entidades, casos de uso e infraestructura claramente separados
-   ExtracciÃ³n de tÃ­tulos de pÃ¡ginas web usando **requests** y **BeautifulSoup**
-   Almacenamiento en JSON mediante un repositorio genÃ©rico
-   Suite de pruebas unitarias con **pytest** para el caso de uso principal

## ðŸ”¹ TecnologÃ­as

-   Python 3.13+
-   requests
-   beautifulsoup4
-   lxml
-   pytest

## ðŸ”¹ InstalaciÃ³n

1. Clona este repositorio:

    ```bash
    git clone <tu-repo-url> webscraper_clean
    cd webscraper_clean
    ```

2. Crea y activa un entorno virtual:

    - Linux/macOS:

        ```bash
        python -m venv venv
        source venv/bin/activate
        ```

    - Windows (PowerShell):

        ```powershell
        python -m venv venv
        venv\\Scripts\\activate
        ```

3. Instala las dependencias:

    ```bash
    pip install -r requirements.txt
    ```

## ðŸ”¹ Uso

Ejecuta el script principal para scrapear URLs y guardar los resultados en JSON:

```bash
python main.py
```

Por defecto, `main.py` procesa las URLs definidas en el arreglo interno y genera el archivo `scraped_pages.json`.

## ðŸ”¹ Estructura del Proyecto

```plaintext
webscraper_clean/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ domain/                  # Entidades y contratos de repositorio
â”‚   â”‚   â”œâ”€â”€ page.py              # Clase Page
â”‚   â”‚   â””â”€â”€ storage_repository.py# Interfaz StorageRepository
â”‚   â”œâ”€â”€ use_cases/               # Casos de uso (logica de negocio)
â”‚   â”‚   â”œâ”€â”€ scrape_page.py       # PageScraper
â”‚   â”‚   â””â”€â”€ scrape_and_store.py  # ScrapeAndStore
â”‚   â”œâ”€â”€ infrastructure/          # Implementaciones concretas
â”‚   â”‚   â”œâ”€â”€ web/                 # Capa de scraping
â”‚
â”‚   â”‚   â”‚   â””â”€â”€ bs_parser.py     # PageParser
â”‚   â”‚   â””â”€â”€ storage/             # Persistencia
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ json_storage.py  # JsonFileStorage
â”‚   â”œâ”€â”€ interfaces/              # Puntos de entrada (CLI, APIs)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â””â”€â”€ config.py                # ConfiguraciÃ³n global
â”œâ”€â”€ tests/                       # Pruebas unitarias
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ unit/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ test_scrape_page.py  # Test de PageScraper
â”œâ”€â”€ main.py                      # Script de ejecuciÃ³n
â”œâ”€â”€ requirements.txt             # Dependencias
â””â”€â”€ README.md                    # Este archivo
```

## ðŸ”¹ Pruebas Unitarias

Ejecuta los tests con:

```bash
export PYTHONPATH=.
pytest -q
```

## ðŸ”¹ Almacenamiento de Datos

Los resultados se guardan en un archivo JSON (`scraped_pages.json`) a travÃ©s de la implementaciÃ³n `JsonFileStorage`, que cumple la interfaz `StorageRepository`.

## ðŸ”¹ Contribuir

1. Haz un fork de este repositorio
2. Crea una rama para tu feature (`git checkout -b feature/nueva-funcionalidad`)
3. Commit de tus cambios (`git commit -m 'AÃ±ade nueva funcionalidad'`)
4. Push a tu rama (`git push origin feature/nueva-funcionalidad`)
5. Abre un Pull Request

## ðŸ”¹ Licencia

Este proyecto estÃ¡ bajo la licencia MIT. Consulta el archivo [LICENSE](LICENSE) para mÃ¡s detalles.
